{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unbiased ECMWF\n",
    "\n",
    "Here we propose a small model which is a debiased ECMWF forecast according to the data we have.\n",
    "The plan is\n",
    "* Compute the bias between the ECMWF model and the observations\n",
    "* Make a debiased model\n",
    "* Turn this model into a probabilistic forecast\n",
    "For this notebook we want to do it on precipitation and temperature, for weeks 1-2, 3-4, and 5-6."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.distributed\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import scipy.stats\n",
    "import typing\n",
    "import xarray as xr\n",
    "import xskillscore as xs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from crims2s.dask import create_dask_cluster\n",
    "from crims2s.util import fix_dataset_dims"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "INPUT_TRAIN = '***BASEDIR***training-input/0.3.0/netcdf'\n",
    "OBSERVATIONS = '***BASEDIR***/processed/training-output-reference/'\n",
    "BENCHNMARK = '***BASEDIR***training-output-benchmark/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Boost dask cluster"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cluster = create_dask_cluster()\n",
    "cluster.scale(jobs=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client = dask.distributed.Client(cluster)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generic Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_train_validation_from_lead_time(xr_data) -> typing.Tuple:\n",
    "    xr_data_sub_train = xr_data.sel(forecast_year=slice(None, 2018))\n",
    "    xr_data_sub_val = xr_data.sel(forecast_year=slice(2019, None))\n",
    "    \n",
    "    return xr_data_sub_train, xr_data_sub_val"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compute_and_correct_bias(data_center_train, data_center_val, obs_train):\n",
    "    \n",
    "    bias = (obs_train - data_center_train).mean(dim=['lead_time', 'forecast_year'])\n",
    "    corrected_bias = data_center_val + bias\n",
    "    \n",
    "    return bias, corrected_bias"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def add_biweekly_dim(dataset):\n",
    "    weeklys = []\n",
    "    for s in [slice('0D', '13D'), slice('14D', '27D'), slice('28D', '41D')]:\n",
    "        weekly_forecast = dataset.sel(lead_time=s)\n",
    "\n",
    "        first_lead = pd.to_timedelta(weekly_forecast.lead_time[0].item())\n",
    "\n",
    "        weekly_forecast = weekly_forecast.expand_dims(dim='biweekly_forecast').assign_coords(biweekly_forecast=[first_lead])\n",
    "        weekly_forecast = weekly_forecast.assign_coords(lead_time=(weekly_forecast.lead_time - first_lead))\n",
    "        weeklys.append(weekly_forecast)\n",
    "        \n",
    "    return xr.concat(weeklys, dim='biweekly_forecast').transpose('forecast_year', 'forecast_dayofyear', 'biweekly_forecast', ...)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ECMWF Temperature"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "CENTER = 'ecmwf'\n",
    "FIELD = 'tp'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_path = pathlib.Path(INPUT_TRAIN)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_files_tp = sorted([f for f in input_path.iterdir() if CENTER in f.stem and FIELD in f.stem])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_files_tp[:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp_raw = xr.open_mfdataset(input_files_tp, preprocess=fix_dataset_dims)\n",
    "ecmwf_tp_raw = ecmwf_tp_raw.assign_coords(lead_time=ecmwf_tp_raw.lead_time - ecmwf_tp_raw.lead_time[0])\n",
    "# Fix the lead times by starting them at 0. To be validated with the organizers."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp = add_biweekly_dim(ecmwf_tp_raw)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obs_path = pathlib.Path(OBSERVATIONS)\n",
    "obs_files = [f for f in obs_path.iterdir() if 'tp' in f.stem]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obs_files[:4]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obs_tp_raw = xr.open_mfdataset(obs_files)\n",
    "obs_tp_raw = obs_tp_raw.assign_coords(lead_time=obs_tp_raw.lead_time - obs_tp_raw.lead_time[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obs_tp = add_biweekly_dim(obs_tp_raw)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obs_tp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For precipitation we first have to take the biweekly total precip. We can't compute the difference directly on the daily forecasts."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp = ecmwf_tp.isel(lead_time=-1) - ecmwf_tp.isel(lead_time=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp.isel(biweekly_forecast=1, forecast_dayofyear=0, forecast_year=0, realization=0).compute().tp.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obs_tp = obs_tp.isel(lead_time=-1) - obs_tp.isel(lead_time=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp_train, ecmwf_tp_val = extract_train_validation_from_lead_time(ecmwf_tp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obs_tp_train, obs_tp_val = extract_train_validation_from_lead_time(obs_tp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obs_tp_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Debiasing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute bias using training data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp_bias = (obs_tp_train - ecmwf_tp_train).mean(dim=['forecast_year'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp_bias"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bias correct ECMWF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp_val_corrected = ecmwf_tp_val + ecmwf_tp_bias"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp_val_corrected"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp_val_corrected_comp = ecmwf_tp_val_corrected.compute()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Turn into probabilistic forecast"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get thresholds from train observations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obs_tp_train_thresholds = obs_tp_train.chunk({'forecast_year': -1}).quantile([0.33, 0.67], dim=['forecast_year'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obs_tp_train_thresholds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obs_tp_train_thresholds_comp = obs_tp_train_thresholds.compute()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute p of thresholds according to the model\n",
    "\n",
    "There are two ways to do this. \n",
    "We can either count the amount of members that are whithin each category.\n",
    "Or compute a distribution of all the members of the model, and then compute the value of the CDF for each threshold.\n",
    "\n",
    "Here we do it using the distribution method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compute a distribution of the members of the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp_val_corrected_mean = ecmwf_tp_val_corrected_comp.mean(dim=['realization'])\n",
    "ecmwf_tp_val_corrected_std = ecmwf_tp_val_corrected_comp.std(dim=['realization'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compute the value of the CDF for each threshold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp_val_corrected_mean"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecmwf_tp_val_corrected_mean.isel(biweekly_forecast=1, forecast_dayofyear=25).tp.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obs_tp_train_thresholds_comp.isel(biweekly_forecast=2, quantile=0, forecast_dayofyear=40).tp.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def make_probabilistic(forecast, thresholds):   \n",
    "    loc = forecast.mean(dim=['realization']).compute().tp\n",
    "    scale = forecast.std(dim=['realization']).compute().tp\n",
    "    \n",
    "    cdfs = xr.apply_ufunc(scipy.stats.norm.cdf, thresholds.tp, dask='allowed', kwargs={'loc': loc, 'scale': scale})\n",
    "    \n",
    "    below = cdfs.isel(quantile=0).drop_vars('quantile')\n",
    "    normal = (cdfs.isel(quantile=1) - cdfs.isel(quantile=0))\n",
    "    above = xr.ones_like(normal) - cdfs.isel(quantile=1).drop_vars('quantile')\n",
    "    \n",
    "    return xr.Dataset({'tp': xr.concat([below, normal, above], 'category').assign_coords(category=['below normal', 'near normal', 'above normal'])})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_probabilistic_forecast = make_probabilistic(ecmwf_tp_val_corrected_comp, obs_tp_train_thresholds_comp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_probabilistic_forecast = val_probabilistic_forecast.expand_dims('forecast_year').assign_coords(forecast_year=ecmwf_tp_val_corrected_comp.forecast_year)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#val_probabilistic_forecast = val_probabilistic_forecast.assign_coords(valid_time=ecmwf_t2m_val_corrected_comp.valid_time)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_probabilistic_forecast.biweekly_forecast.data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_probabilistic_forecast"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_probabilistic_forecast = val_probabilistic_forecast.rename_dims({'biweekly_forecast': 'lead_time'}).assign_coords(lead_time=val_probabilistic_forecast.biweekly_forecast.data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_probabilistic_forecast"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_probabilistic_forecast.to_netcdf('***BASEDIR***/test_tp_forecast.nc')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_probabilistic_forecast.isel(category=2, forecast_dayofyear=40, lead_time=1).tp.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_probabilistic_forecast.isel(category=1, forecast_dayofyear=40, biweekly_forecast=0).plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sanity check"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_probabilistic_forecast.sum(dim='category').isel(forecast_dayofyear=0, lead_time=2).tp.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make submission file out of it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_probabilistic_forecast_unfixed = val_probabilistic_forecast.stack(forecast_time=['forecast_year', 'forecast_dayofyear'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_probabilistic_forecast_unfixed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "forecast_times = []\n",
    "for f in val_probabilistic_forecast_unfixed.forecast_time:\n",
    "    year, dayofyear = f.data.item()\n",
    "    year = pd.to_datetime(f'{year}-01-01')\n",
    "    dayofyear = pd.Timedelta(dayofyear - 1, 'D')\n",
    "    forecast_times.append(year + dayofyear)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "forecast_time = xr.DataArray(forecast_times, dims='forecast_time')\n",
    "val_probabilistic_forecast_unfixed.assign_coords(forecast_time=forecast_time).to_netcdf('***BASEDIR***/test_tp_forecast.nc')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S2S Challenge",
   "language": "python",
   "name": "s2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}