{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import pathlib\n",
    "import xarray as xr\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from crims2s.util import fix_dataset_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVATIONS = '***BASEDIR***training-output-reference/'\n",
    "BENCHNMARK = '***BASEDIR***training-output-benchmark/'\n",
    "CENTER = 'ecmwf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '***BASEDIR***training-input/'\n",
    "OUTPUT_DIR = '***BASEDIR***processed/training-input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '***BASEDIR***test-input/'\n",
    "OUTPUT_DIR = '***BASEDIR***processed/test-input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_rechunk = ['t', 'gh', 'u', 'v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = pathlib.Path(INPUT_DIR)\n",
    "output_path = pathlib.Path(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = []\n",
    "for field in to_rechunk:\n",
    "    input_files.extend([f for f in input_path.iterdir() if CENTER in f.stem and f'-{field}-' in f.stem])\n",
    "    \n",
    "input_files = sorted(input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*_, field, datestring] = input_files[0].stem.split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_one_file(f):\n",
    "    new_file = output_path / f.name\n",
    "    [*_, field, datestring] = f.stem.split('-')\n",
    "    \n",
    "    d = xr.open_dataset(f)\n",
    "    \n",
    "    #for level in d.plev:\n",
    "    for level in [1000, 200, 850]:\n",
    "        level_str = int(level)\n",
    "        new_filename = f'{CENTER}-hindcast-{field}{level_str}-{datestring}.nc'\n",
    "        new_path = output_path / new_filename\n",
    "        \n",
    "        #if new_path.is_file():\n",
    "        #    print(f'Skipping {new_path} because it exists.')\n",
    "        #    return\n",
    "        \n",
    "        subset = d.sel(plev=[level])\n",
    "        #print(new_path)\n",
    "        \n",
    "        subset.to_netcdf(new_path, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "do_one_file(input_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with multiprocessing.Pool(processes=6) as pool:\n",
    "    for _ in tqdm(pool.imap(do_one_file, input_files), total=len(input_files)):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in input_files:\n",
    "    new_file = output_path / f.name\n",
    "    datestring = f.stem.split('-')[-1]\n",
    "    \n",
    "    print(new_file)\n",
    "    d = xr.open_dataset(f)\n",
    "    d = fix_dataset_dims(d)\n",
    "    \n",
    "    for level in d.plev:\n",
    "        level_str = int(level.item())\n",
    "        new_filename = f'{CENTER}-hindcast-{FIELD}{level_str}-{datestring}.nc'\n",
    "        new_path = output_path / new_filename\n",
    "        subset = d.sel(plev=[level])\n",
    "        print(new_path)\n",
    "        \n",
    "        subset.to_netcdf(new_path, mode='w')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = sorted([f for f in output_path.iterdir() if '20200102' in f.stem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_dataset(output_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_mfdataset(output_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S2S Challenge",
   "language": "python",
   "name": "s2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
