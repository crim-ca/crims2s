{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validation Notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import xarray as xr\n",
    "import xskillscore as xs\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xr.set_options(keep_attrs=True)\n",
    "#xr.set_options(display_style='text')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "SHOW_VISUALISATIONS: bool = False\n",
    "\n",
    "EXPORT_DATA: bool = False\n",
    "    \n",
    "COMPUTE_EACH_TROPICS: bool = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "OUTPUT = '***BASEDIR***'\n",
    "OUTPUT = \"***BASEDIR***/last-validation-files-from-organizers\"\n",
    "\n",
    "EXPORT_PATH = '***HOME***Projets/S2S-Competition/outputs'\n",
    "CMAP = \"coolwarm\"\n",
    "\n",
    "CMAP = \"RdYlGn\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validation class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "\n",
    "class Validation():\n",
    "    def __init__(self, obs: xr.Dataset, preds: xr.Dataset, use_dry_mask=False) -> None:\n",
    "        \n",
    "       \n",
    "        self.obs: xr.Dataset = obs\n",
    "        self.preds: xr.Dataset = preds\n",
    "        self.clim_p: xr.DataArray = xr.DataArray()\n",
    "            \n",
    "        self.rps_ml: xr.Dataset = xr.Dataset()\n",
    "        self.rps_clim: xr.Dataset = xr.Dataset()\n",
    "            \n",
    "        self.regions_positions: dict = {'North_America': (slice(0, 50), slice(120, 300)),\n",
    "                                        'South_America': (slice(40, 100), slice(150, 300)),\n",
    "                                        'Europe_Africa_Asia': (slice(0, 100), slice(0, 60)),\n",
    "                                        'Oceania': (slice(50, 100), slice(70, 120))}\n",
    "            \n",
    "        self.regions_positions_orthographic: dict = {'North_America': (-80, 25),\n",
    "                                                     'South_America': (-60, -15),\n",
    "                                                     'Europe_Africa_Asia': (50, 25),\n",
    "                                                     'Oceania': (120, 1)}\n",
    "\n",
    "        self.__initialise_climatology_probabilities()\n",
    "        self.__assert_predictions_2020(self.obs)\n",
    "        self.__assert_predictions_2020(self.preds)\n",
    "        \n",
    "        if use_dry_mask:\n",
    "            dryval = self.preds.copy(deep=True)\n",
    "            drymask = (self.preds.tp.sel(category=\"above normal\") < 1).data\n",
    "            print(\"Before\", dryval.tp.sel(category=\"above normal\").data[drymask])\n",
    "            dryval.tp.sel(category=\"above normal\").data[drymask] = 0.0          \n",
    "            print(\"After\", dryval.tp.sel(category=\"above normal\").data[drymask])          \n",
    "            self.preds.tp.sel(category=\"above normal\").data[drymask] = dryval.tp.sel(category=\"above normal\").data[drymask]\n",
    "            print(\"New tp values\", self.preds.tp.sel(category=\"above normal\").data[drymask])\n",
    "\n",
    "        print(\"Validation class successfully initialised\")\n",
    "        \n",
    "                 \n",
    "    def __initialise_climatology_probabilities(self) -> None:\n",
    "        \"\"\"\n",
    "        REFORMAT FROM ORGANIZERS NOTEBOOKS\n",
    "        Initialize the climatology DataArray to 1/3 values for each categories\n",
    "        \"\"\"\n",
    "        self.clim_p = xr.DataArray([1/3, 1/3, 1/3], dims='category', coords={'category':['below normal', 'near normal', 'above normal']}).to_dataset(name='tp')\n",
    "        self.clim_p['t2m'] = self.clim_p['tp']\n",
    "        \n",
    "        \n",
    "    def __assert_predictions_2020(self, preds_test) -> None:\n",
    "        \"\"\"\n",
    "        REFORMAT FROM ORGANIZERS NOTEBOOKS\n",
    "        Check the variables, coordinates and dimensions of 2020 predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # is dataset\n",
    "        assert isinstance(preds_test, xr.Dataset)\n",
    "\n",
    "        # has both vars: tp and t2m\n",
    "        assert 'tp' in preds_test.data_vars\n",
    "        assert 't2m' in preds_test.data_vars\n",
    "\n",
    "        ## coords\n",
    "        # forecast_time\n",
    "        d = pd.date_range(start='2020-01-02', freq='7D', periods=53)\n",
    "        forecast_time = xr.DataArray(d, dims='forecast_time', coords={'forecast_time':d})\n",
    "        assert (forecast_time == preds_test['forecast_time']).all()\n",
    "\n",
    "        # longitude\n",
    "        lon = np.arange(0., 360., 1.5)\n",
    "        longitude = xr.DataArray(lon, dims='longitude', coords={'longitude': lon})\n",
    "        assert (longitude == preds_test['longitude']).all()\n",
    "\n",
    "        # latitude\n",
    "        lat = np.arange(90., -90.1, 1.5)\n",
    "        latitude = xr.DataArray(lat, dims='latitude', coords={'latitude': lat})\n",
    "        assert (latitude == preds_test['latitude']).all()\n",
    "        \n",
    "        # LATITUDE DEFINITION ACCORDING TO SCORING SCRIPT\n",
    "        #assert (preds_test.latitude.diff('latitude')==-1.5).all()\n",
    "        #assert 60 in preds_test.latitude\n",
    "        #assert -60 in preds_test.latitude\n",
    "\n",
    "        # lead_time\n",
    "        lead = [pd.Timedelta(f'{i} d') for i in [14, 28]]\n",
    "        lead_time = xr.DataArray(lead, dims='lead_time', coords={'lead_time': lead})\n",
    "        assert (lead_time == preds_test['lead_time']).all()\n",
    "\n",
    "        # category\n",
    "        cat = np.array(['below normal', 'near normal', 'above normal'], dtype='<U12')\n",
    "        category = xr.DataArray(cat, dims='category', coords={'category': cat})\n",
    "        assert (category == preds_test['category']).all()\n",
    "\n",
    "        # size\n",
    "        #from dask.utils import format_bytes\n",
    "        #size_in_MB = float(format_bytes(preds_test.nbytes).split(' ')[0])\n",
    "        #assert size_in_MB > 50\n",
    "        #assert size_in_MB < 250\n",
    "\n",
    "        # no other dims\n",
    "        assert set(preds_test.dims) - {'category', 'forecast_time', 'latitude', 'lead_time', 'longitude'} == set()\n",
    "        \n",
    "        \n",
    "    def __compute_RPS(self, preds, obs, clim_p):   \n",
    "        rps_ml = xs.rps(obs, preds, category_edges=None, dim='forecast_time', input_distributions='p').compute()  \n",
    "        rps_clim = xs.rps(obs, clim_p, category_edges=None, dim='forecast_time', input_distributions='p').compute()      \n",
    "        return rps_ml, rps_clim\n",
    "            \n",
    "    def plot_rps(self) -> None:\n",
    "        \n",
    "        rps_ml, rps_clim = self.__compute_RPS(self.preds, self.obs, self.clim_p)\n",
    "        \n",
    "        for v in rps_ml.data_vars:\n",
    "            data_var = rps_clim[v]\n",
    "            data_var.where(data_var.latitude > -60., drop=True).plot(robust=True, col='lead_time', figsize=(10,5))\n",
    "\n",
    "        for v in self.rps_clim.data_vars:\n",
    "            data_var = rps_clim[v]\n",
    "            data_var.where(data_var.latitude > -60., drop=True).plot(robust=True, col='lead_time', figsize=(10,5))\n",
    "        \n",
    "    def __compute_tropic_mask(self, rpss: xr.Dataset) -> xr.Dataset:\n",
    "        \n",
    "        mask = xr.ones_like(rpss.isel(lead_time=0, drop=True)).reset_coords(drop=True).t2m\n",
    "        boundary_tropics = 30\n",
    "        mask = xr.concat([mask.where(mask.latitude > boundary_tropics),\n",
    "                          mask.where(np.abs(mask.latitude) <= boundary_tropics),\n",
    "                          mask.where((mask.latitude < -boundary_tropics) & (mask.latitude > -60))], 'area')\n",
    "        mask = mask.assign_coords(area=['northern_extratropics', 'tropics', 'southern_extratropics'])\n",
    "        mask.name = 'area'\n",
    "\n",
    "        return mask.where(rpss.t2m.isel(lead_time=0, drop=True).notnull())\n",
    "    \n",
    "    def __compute_each_tropic_mask(self, rpss: xr.Dataset) -> xr.Dataset:\n",
    "        \n",
    "        mask = xr.ones_like(rpss.isel(lead_time=0, drop=True)).reset_coords(drop=True).t2m\n",
    "\n",
    "        it = [i for i in range(int(self.preds.latitude.data.min()), int(self.preds.latitude.data.max()) + 1, 10)]\n",
    "        mask_tropics = list()\n",
    "        tropic_name = list()\n",
    "        for i in range(len(it)):      \n",
    "            if i + 1 < len(it):\n",
    "                up = it[i+1]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            tropic_name.append(str(it[i]) + \" - \" + str(up))\n",
    "            mask_tropics.append(mask.where((mask.latitude >= it[i]) & (mask.latitude < up)))\n",
    "\n",
    "        mask = xr.concat(mask_tropics, 'area')\n",
    "        mask = mask.assign_coords(area=tropic_name)\n",
    "        mask.name = 'area'\n",
    "        \n",
    "        return mask.where(rpss.t2m.isel(lead_time=0, drop=True).notnull())\n",
    "    \n",
    "    def __compute_rpss_organizers(self, rps_ml, rps_clim) -> xr.Dataset:\n",
    "        return (1 - rps_ml / rps_clim)\n",
    "    \n",
    "    def compute_scores_from_scoring_image(self, dry_mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute scores from scoring image provide by organizers\n",
    "        CARREFUL : when assert prediction, latitude are not define as Renku repository !!!\n",
    "        \"\"\"\n",
    "        \n",
    "        # submission rps\n",
    "        rps_ML = xs.rps(self.obs, self.preds, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "        # climatology forecast rps\n",
    "        rps_clim = xs.rps(self.obs, self.clim_p, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "        \n",
    "        self.rpl_ML = rps_ML\n",
    "        self.rps_clim = rps_clim\n",
    "\n",
    "        # submission rpss wrt climatology\n",
    "        rpss = (1 - rps_ML / rps_clim)\n",
    "\n",
    "        # https://renkulab.io/gitlab/aaron.spring/s2s-ai-challenge-template/-/issues/7\n",
    "        # penalize\n",
    "        penalize = self.obs.where(self.preds!=1, other=-10).mean('category')\n",
    "        rpss = rpss.where(penalize!=0,other=-10)\n",
    "\n",
    "        # clip\n",
    "        rpss = rpss.clip(-10, 1)\n",
    "\n",
    "        # average over all forecasts\n",
    "        rpss = rpss.mean('forecast_time')\n",
    "        \n",
    "        \n",
    "\n",
    "        # weighted area mean\n",
    "        weights = np.cos(np.deg2rad(np.abs(rpss.latitude)))\n",
    "        # spatially weighted score averaged over lead_times and variables to one single value\n",
    "        scores = rpss.sel(latitude=slice(None, -60)).weighted(weights).mean('latitude').mean('longitude')\n",
    "        scores = scores.to_array().mean(['lead_time', 'variable']).reset_coords(drop=True)\n",
    "        # score transfered to leaderboard\n",
    "        return scores.item()\n",
    "          \n",
    "    def compute_scores_from_organizers_RPSS(self, compute_each_tropics=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        REFORMAT FROM ORGANIZERS NOTEBOOKS\n",
    "        Compute RPSS according to notebook organizers\n",
    "        \"\"\"\n",
    "        \n",
    "        rps_ml, rps_clim = self.__compute_RPS(self.preds, self.obs, self.clim_p)\n",
    "        \n",
    "        rpss_organizers = self.__compute_rpss_organizers(rps_ml, rps_clim)\n",
    "        \n",
    "        mask = (self.__compute_each_tropic_mask(rps_ml) if compute_each_tropics else self.__compute_tropic_mask(rps_ml))\n",
    "        weights = np.cos(np.deg2rad(np.abs(mask.latitude)))\n",
    "        \n",
    "        scores = (rpss_organizers*mask).weighted(weights).mean('latitude').mean('longitude')\n",
    "        \n",
    "        return scores.reset_coords(drop=True).to_dataframe().unstack(0).T.round(2)\n",
    "        \n",
    "    def compute_scores_from_arlan_RPSS(self, compute_each_tropics=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute RPSS according to Arlan's definition\n",
    "        \"\"\"\n",
    "        \n",
    "        rps_ml, rps_clim = self.__compute_RPS(self.preds, self.obs, self.clim_p)\n",
    "        \n",
    "        mask = (self.__compute_each_tropic_mask(rps_ml) if compute_each_tropics else self.__compute_tropic_mask(rps_ml))\n",
    "        weights = np.cos(np.deg2rad(np.abs(mask.latitude)))\n",
    "        \n",
    "        latitude_longitude_mean_ML = (rps_ml*mask).weighted(weights).mean(dim=['latitude', 'longitude'])\n",
    "        latitude_longitude_mean_clim = (rps_clim*mask).weighted(weights).mean(dim=['latitude', 'longitude'])\n",
    "        \n",
    "        rpss_arlan = 1 - latitude_longitude_mean_ML/latitude_longitude_mean_clim\n",
    "                \n",
    "        return rpss_arlan.reset_coords(drop=True).to_dataframe().unstack(0).T.round(2)\n",
    "    \n",
    "    def __extract_from_month(self, xr_data: xr.Dataset, month: int) -> xr.Dataset:\n",
    "        mask = xr_data.forecast_time.dt.month == month        \n",
    "        return xr_data.sel(forecast_time=mask)\n",
    "    \n",
    "    def compute_monthly_scores(self, compute_each_tropics:bool = False) -> pd.DataFrame:\n",
    "        \n",
    "        scores_dict: dict = dict()\n",
    "        date_list:list = list()\n",
    "        for month in range(1, 13):                       \n",
    "            #date = (str(\"2020-0\") + str(month+1) if month + 1 <= 9 else str(\"2020-\") + str(month+1))\n",
    "            \n",
    "            obs = self.__extract_from_month(xr_data = self.obs, month=month)\n",
    "            preds = self.__extract_from_month(xr_data = self.preds, month=month)          \n",
    "            rps_ml, rps_clim = self.__compute_RPS(preds, obs, self.clim_p)          \n",
    "            rpss_organizers = self.__compute_rpss_organizers(rps_ml, rps_clim)\n",
    "            \n",
    "            mask = (self.__compute_each_tropic_mask(rps_ml) if compute_each_tropics else self.__compute_tropic_mask(rps_ml))\n",
    "            weights = np.cos(np.deg2rad(np.abs(mask.latitude)))\n",
    "            \n",
    "            scores = ((rpss_organizers*mask).weighted(weights).mean('latitude').mean('longitude') if compute_each_tropics else rpss_organizers.weighted(weights).mean('latitude').mean('longitude')) \n",
    "                \n",
    "            scores_dict[month] = scores.reset_coords(drop=True).to_dataframe().unstack(0).T.round(2)\n",
    "            date_list.append(month)\n",
    "         \n",
    "        scores = pd.concat(list(scores_dict.values()), axis=1)\n",
    "        \n",
    "        if compute_each_tropics:\n",
    "            index = [(month, (timed[0], timed[1])) for month, timed in zip(date_list, [(scores.columns[0], scores.columns[1]) for i in range(24)])]\n",
    "            \n",
    "            new_index = list()\n",
    "            for item in index:\n",
    "                new_index.append((item[0], item[1][0]))\n",
    "                new_index.append((item[0], item[1][1]))\n",
    "\n",
    "            scores.columns = pd.MultiIndex.from_tuples(new_index)\n",
    "        else:\n",
    "            scores.columns = date_list\n",
    "            \n",
    "        return scores\n",
    "    \n",
    "    def compute_trimestrial_scores(self):\n",
    "        \n",
    "        first_trimestrial = (\"2020-01\", \"2020-04\")\n",
    "        second_trimestrial= (\"2020-05\", \"2020-08\")\n",
    "        third_trimestrial = (\"2020-09\", \"2020-12\")\n",
    "        \n",
    "        \n",
    "              \n",
    "        scores_list: list = list()\n",
    "        for trimestrial in range(1, 13, 4):\n",
    "            position = ((self.preds.forecast_time.dt.month >= trimestrial) & \n",
    "                        (self.preds.forecast_time.dt.month < trimestrial + 4))\n",
    "                        \n",
    "            obs = self.obs.isel(forecast_time=position)\n",
    "            preds = self.preds.isel(forecast_time=position)\n",
    "            rps_ml, rps_clim = self.__compute_RPS(preds, obs, self.clim_p)\n",
    "            rpss_organizers = self.__compute_rpss_organizers(rps_ml, rps_clim)\n",
    "            mask = self.__compute_tropic_mask(rps_ml)      \n",
    "            weights = np.cos(np.deg2rad(np.abs(mask.latitude)))\n",
    "            scores = rpss_organizers.weighted(weights).mean('latitude').mean('longitude')\n",
    "            scores_list.append(scores.reset_coords(drop=True).to_dataframe().unstack(0).T.round(2))\n",
    "            \n",
    "        scores_list = pd.concat(scores_list, axis=1)\n",
    "        scores_list.columns = [\"January to April\", \"May to August\", \"September to December\"]\n",
    "        return scores_list\n",
    "    \n",
    "    \n",
    "    def plot_by_region(self, what:str='observations', feat:str='t2m', region:str='North_America', category:int=2, forecast_time:int=0, orthographic: bool=False):\n",
    "        \n",
    "        assert 't2m' or 'tp' in feat\n",
    "        assert region in list(val.regions_positions.keys())\n",
    "        assert category <= 2\n",
    "        \n",
    "        print('Plot', feat,'for category nÂ°', category, 'for', what, 'data for', region, 'for forecast_time', forecast_time)\n",
    "        \n",
    "        latitude: slice = self.regions_positions[region][0]\n",
    "        longitude: slice = self.regions_positions[region][1]\n",
    "            \n",
    "        orthographic_pos = self.regions_positions_orthographic[region]\n",
    "                    \n",
    "        if what == 'observations':\n",
    "            if not orthographic:\n",
    "                self.obs[feat].isel(category=category, \n",
    "                                    forecast_time=forecast_time, \n",
    "                                    latitude=latitude, \n",
    "                                    longitude=longitude).plot(col='lead_time', figsize=(10,5))\n",
    "            else:\n",
    "                self.obs[feat].isel(category=category, \n",
    "                                    forecast_time=forecast_time).plot(col='lead_time', subplot_kws=dict(projection=ccrs.Orthographic(orthographic_pos[0], orthographic_pos[1]), facecolor=\"gray\"),\n",
    "                                                                     transform=ccrs.PlateCarree(), \n",
    "                                                                     figsize=(20,20))\n",
    "        elif what == 'predictions':\n",
    "            if not orthographic:\n",
    "                self.preds[feat].isel(category=category, \n",
    "                                      forecast_time=forecast_time, \n",
    "                                      latitude=latitude, \n",
    "                                      longitude=longitude).plot(col='lead_time', figsize=(10,5))\n",
    "            else:\n",
    "                self.preds[feat].isel(category=category, \n",
    "                                    forecast_time=forecast_time).plot(col='lead_time', subplot_kws=dict(projection=ccrs.Orthographic(orthographic_pos[0], orthographic_pos[1]), facecolor=\"gray\"),\n",
    "                                                                      transform=ccrs.PlateCarree(),\n",
    "                                                                      figsize=(20,20))\n",
    "        else:\n",
    "            print(what, 'is not in possible visualisation. Try observations or predictions')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Open and preprocess Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#obs = xr.open_dataset(\"***BASEDIR***/renku/hindcast-like-observations_2018-2019_biweekly_terciled.nc\")\n",
    "#preds = xr.open_dataset(\"***BASEDIR***/ml-output/2021-08-25-emos-normal-gamma-no-multiplex-without-validtime-weeks36.nc\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#preds = xr.open_dataset(\"***BASEDIR***/runs/infer/outputs/2021-09-07/14-28-19/normal_normal_monthly.nc\")\n",
    "#obs = xr.open_dataset(\"***BASEDIR***/last-validation-files-from-organizers/forecast-like-observations_2020_biweekly_terciled.nc\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preds = xr.open_dataset(\"***BASEDIR***/runs/infer/outputs/2021-09-13/10-22-18/bayes_linear.nc\")\n",
    "obs = xr.open_dataset(\"***BASEDIR***/last-validation-files-from-organizers/forecast-like-observations_2020_biweekly_terciled.nc\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# original 2020 observations : \"***BASEDIR***/last-validation-files-from-organizers/forecast-like-observations_2020_biweekly_terciled.nc\"\n",
    "# original 2020 ML Model : \"***BASEDIR***/last-validation-files-from-organizers/forecast-like-observations_2020_biweekly_terciled.nc\"\n",
    "\n",
    "# obs = xr.open_dataset(\"***BASEDIR***/last-validation-files-from-organizers/forecast-like-observations_2020_biweekly_terciled.nc\")\n",
    "# preds = xr.open_dataset(\"***BASEDIR***/ml-output/2021-08-25-emos-normal-gamma-no-multiplex-without-validtime-weeks36.nc\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val = Validation(obs=obs, preds=preds, use_dry_mask = False)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute scores for organizers scoring image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val.plot_by_region(what='predictions', region='North_America', orthographic=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val.plot_by_region(what='predictions', region='Europe_Africa_Asia', orthographic=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val.plot_by_region(what='predictions', region='Oceania', orthographic=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val.plot_rps()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val.compute_scores_from_scoring_image()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute RPPS from organizers and Arlan's formula"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "organizers_rpss = val.compute_scores_from_organizers_RPSS(compute_each_tropics=COMPUTE_EACH_TROPICS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "arlan_rpss = val.compute_scores_from_arlan_RPSS(compute_each_tropics=COMPUTE_EACH_TROPICS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rpss = pd.concat([organizers_rpss, arlan_rpss], axis=1, keys=['Organizer RPSS', 'Arlan RPSS'])\n",
    "rpss.style.background_gradient(cmap=CMAP, vmin=-np.abs([rpss.min().min(), rpss.max().max()]).max(), vmax=np.abs([rpss.min().min(), rpss.max().max()]).max()).set_precision(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute and plot monthly scores"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here you can set keep_tropics to True to compute scores in function of tropics defined by organizers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores_monthly = val.compute_monthly_scores(compute_each_tropics=COMPUTE_EACH_TROPICS)\n",
    "scores_monthly.style.background_gradient(cmap=CMAP, vmin=-np.abs([rpss.min().min(), rpss.max().max()]).max(), vmax=np.abs([rpss.min().min(), rpss.max().max()]).max()).set_precision(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(25, 5))\n",
    "scores_monthly.transpose().plot(ax=axes[0])\n",
    "scores_monthly.xs('t2m', axis=0).transpose().plot(ax=axes[1])\n",
    "scores_monthly.xs('tp', axis=0).transpose().plot(ax=axes[2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute and plot trimestrial scores"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores_trimestrials = val.compute_trimestrial_scores()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores_trimestrials.style.background_gradient(cmap=CMAP, vmin=-np.abs([rpss.min().min(), rpss.max().max()]).max(), vmax=np.abs([rpss.min().min(), rpss.max().max()]).max()).set_precision(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(25, 5))\n",
    "scores_trimestrials.transpose().plot(ax=axes[0]) \n",
    "scores_trimestrials.xs('t2m', axis=0).transpose().plot(ax=axes[1])\n",
    "scores_trimestrials.xs('tp', axis=0).transpose().plot(ax=axes[2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export Notebook and figures"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "EXPORT_DATA = False\n",
    "if EXPORT_DATA:\n",
    "    \n",
    "    save_date = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S%f')[:-3]\n",
    "    \n",
    "    save_directory: str = os.path.join(EXPORT_PATH, save_date)\n",
    "    os.mkdir(save_directory)\n",
    "    \n",
    "    writer = pd.ExcelWriter(os.path.join(save_directory, \"scores.xlsx\"), engine='xlsxwriter')\n",
    "    scores_monthly.to_excel(writer, sheet_name='Monthly scores')\n",
    "    scores_trimestrials.to_excel(writer, sheet_name='Trimestrial scores')\n",
    "    organizers_rpss.to_excel(writer, sheet_name='Organizers RPSS')\n",
    "    arlan_rpss.to_excel(writer, sheet_name='Arlan RPSS')\n",
    "    writer.save()\n",
    "    \n",
    "    !jupyter nbconvert \"JG - Validation.ipynb\" --to html --output \"Validation Notebook.html\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val.preds.tp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dry_val = val.preds.copy(deep=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "drymask = (dry_val.tp.sel(category=\"above normal\") < 1.0).data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dry_val.tp.sel(category=\"above normal\").data[drymask]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dry_val.tp.sel(category=\"above normal\").data[drymask] = 0.0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dry_val.tp.sel(category=\"above normal\").data[drymask]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val.preds.tp.sel(category=\"above normal\").data[drymask] = dry_val.tp.sel(category=\"above normal\").data[drymask]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val.preds.tp.sel(category=\"above normal\").data[drymask]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S2S Challenge",
   "language": "python",
   "name": "s2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}