{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Rework the model multiplexer notebook so that we include an inference part that produces prediction files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections.abc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.distributions\n",
    "import torch.nn as nn\n",
    "import torch.utils.data.dataloader\n",
    "from typing import Union, Callable, Any, Hashable\n",
    "import xarray as xr\n",
    "\n",
    "from crims2s.dataset import S2SDataset, TransformedDataset\n",
    "from crims2s.transform import CompositeTransform, add_biweekly_dim_transform, add_metadata, example_to_pytorch\n",
    "from crims2s.util import ECMWF_FORECASTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = '***BASEDIR***/mlready/2021-08-08-test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make transform to interface dataset w/ linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_estimator(dataset, dim=None):\n",
    "    dataset_mean = dataset.mean(dim=dim)\n",
    "    \n",
    "    if dim is None:\n",
    "        dim_sizes = [dataset.sizes[x] for x in dataset_mean.dims]\n",
    "    elif isinstance(dim, str):\n",
    "        dim_sizes = dataset.sizes[dim]\n",
    "    else:\n",
    "        dim_sizes = [dataset.sizes[x] for x in dim]\n",
    "    \n",
    "    n = np.prod(dim_sizes)\n",
    "    \n",
    "    return xr.ufuncs.sqrt(xr.ufuncs.square(dataset - dataset_mean).sum(dim=dim) / (n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_to_distribution(model):   \n",
    "    model_tp_mean = model.tp.isel(lead_time=-1).mean(dim='realization').rename('tp_mu')\n",
    "    model_tp_std = std_estimator(model.tp.isel(lead_time=-1), dim='realization').rename('tp_sigma')\n",
    "    \n",
    "    model_t2m_mean = model.t2m.mean(dim=['lead_time', 'realization']).rename('t2m_mu')\n",
    "    model_t2m_std = std_estimator(model.t2m, dim=['lead_time', 'realization']).rename('t2m_sigma')\n",
    "    \n",
    "    return xr.merge([\n",
    "        model_tp_mean, model_tp_std, model_t2m_mean, model_t2m_std\n",
    "    ]).drop('lead_time').rename(biweekly_forecast='lead_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_biweekly(obs):\n",
    "    aggregate_obs_tp = obs.pr.sum(dim='lead_time', min_count=2).rename('tp')\n",
    "    aggregate_obs_t2m = obs.t2m.mean(dim='lead_time')\n",
    "    return xr.merge([aggregate_obs_tp, aggregate_obs_t2m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_adapter(example):\n",
    "    example['model'] = model_to_distribution(example['model'])\n",
    "    example['obs'] = obs_to_biweekly(example['obs'])\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_re = re.compile('01[0-9]{2}.nc$')\n",
    "#filter_re = re.compile('0109.nc$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset = S2SDataset(DATASET, include_features=False, name_filter=lambda x: filter_re.search(x), years=list(range(2000,2017)))\n",
    "raw_val_dataset = S2SDataset(DATASET, include_features=False, name_filter=lambda x: filter_re.search(x), years=list(range(2017,2020)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = CompositeTransform([add_biweekly_dim_transform, linear_model_adapter, add_metadata, example_to_pytorch])\n",
    "\n",
    "train_dataset = TransformedDataset(raw_train_dataset, transform)\n",
    "val_dataset = TransformedDataset(raw_val_dataset, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, num_workers=4, batch_size=None, batch_sampler=None)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, num_workers=4, batch_size=None, batch_sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMultiplexer(nn.Module):\n",
    "    \"\"\"Dispatch the training examples to multiple models depending on the example.\n",
    "    For instance, we could use this to use a different model for every monthday forecast.\n",
    "    \n",
    "    Because it uses an arbitraty model for every sample, this module does not support batching.\n",
    "    To use it, it is recommended to disable automatic batching on the dataloader.\"\"\"\n",
    "    def __init__(self, key, models):\n",
    "        \"\"\"Args:\n",
    "            key: If a str, used as a key to fetch the model name from the example dict. \n",
    "                 If a callable, called on the example and should return to model name to use.\n",
    "            models: A mapping from model names to model instances. They keys should correspond to what is returned when applying key on the example.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        if isinstance(key, str):\n",
    "            self.key_fn = lambda x: x[key]\n",
    "        else:\n",
    "            self.key_fn = key\n",
    "            \n",
    "        self.models = nn.ModuleDict(models)\n",
    "        \n",
    "    def forward(self, example):      \n",
    "        model_name = self.key_fn(example)\n",
    "        model = self.models[model_name]\n",
    "        \n",
    "        return model(example)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, *shape, fill_weights=0.0, fill_intercept=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.weights = nn.Parameter(torch.full(shape, fill_weights))\n",
    "        self.intercept = nn.Parameter(torch.full(shape, fill_intercept))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.intercept + self.weights * x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempPrecipEMOS(nn.Module):\n",
    "    def __init__(self, biweekly=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        shape = (3, 121, 240) if biweekly else (121, 240)\n",
    "        \n",
    "        self.tp_mu_model = LinearModel(*shape)\n",
    "        self.tp_sigma_model = LinearModel(*shape, fill_intercept=1.0)\n",
    "        \n",
    "        self.t2m_mu_model = LinearModel(*shape)\n",
    "        self.t2m_sigma_model = LinearModel(*shape, fill_intercept=1.0)\n",
    "        \n",
    "    def forward(self, example):\n",
    "        forecast_tp_mu, forecast_tp_sigma = example['model_tp_mu'], example['model_tp_sigma']\n",
    "        forecast_t2m_mu, forecast_t2m_sigma = example['model_t2m_mu'], example['model_t2m_sigma']\n",
    "        \n",
    "        tp_mu = self.tp_mu_model(forecast_tp_mu)\n",
    "        tp_sigma = self.tp_sigma_model(forecast_tp_sigma)\n",
    "        tp_sigma = torch.clip(tp_sigma, min=1e-6)\n",
    "\n",
    "        t2m_mu = self.t2m_mu_model(forecast_t2m_mu)\n",
    "        t2m_sigma = self.t2m_sigma_model(forecast_t2m_sigma)\n",
    "        t2m_sigma = torch.clip(t2m_sigma, min=1e-6)\n",
    "        \n",
    "        tp_dist = torch.distributions.Normal(loc=tp_mu, scale=tp_sigma)\n",
    "        t2m_dist = torch.distributions.Normal(loc=t2m_mu, scale=t2m_sigma)\n",
    "        \n",
    "        return t2m_dist, tp_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthdays = [f'{m:02}{d:02}' for m, d in ECMWF_FORECASTS]\n",
    "weekly_models = {monthday: TempPrecipEMOS(biweekly=True) for monthday in monthdays}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelMultiplexer('monthday', weekly_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    train_losses = []\n",
    "    train_temperature_losses = []\n",
    "    train_rain_losses = []\n",
    "    \n",
    "    model.train()\n",
    "    for example in train_dataloader:\n",
    "        t2m_dist, tp_dist = model.forward(example)\n",
    "\n",
    "        tp_obs = example['obs_tp']\n",
    "        tp_nan_mask = tp_obs.isnan()\n",
    "        tp_obs[tp_nan_mask] = 0.0\n",
    "        tp_log_likelihood = tp_dist.log_prob(tp_obs)\n",
    "        tp_log_likelihood[tp_nan_mask] = 0.0\n",
    "\n",
    "        t2m_obs = example['obs_t2m']\n",
    "        t2m_nan_mask = t2m_obs.isnan()\n",
    "        t2m_obs[t2m_nan_mask] = 0.0\n",
    "        t2m_log_likelihood = t2m_dist.log_prob(t2m_obs)\n",
    "        t2m_log_likelihood[t2m_nan_mask] = 0.0\n",
    "\n",
    "        rain_loss = -tp_log_likelihood.mean()\n",
    "        temperature_loss =  -t2m_log_likelihood.mean()\n",
    "        loss = rain_loss + temperature_loss\n",
    "        \n",
    "        loss.backward()\n",
    "                \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_losses.append(float(loss.detach()))\n",
    "        train_temperature_losses.append(float(temperature_loss.detach()))\n",
    "        train_rain_losses.append(float(rain_loss.detach()))\n",
    "\n",
    "    train_mean_loss = np.array(train_losses).mean()\n",
    "    train_mean_rain_loss = np.array(train_rain_losses).mean()\n",
    "    train_mean_temperature_loss = np.array(train_temperature_losses).mean()\n",
    "    print(f'Epoch {epoch} train loss: {train_mean_loss}. Temperature: {train_mean_temperature_loss}. Rain: {train_mean_rain_loss}.')\n",
    "        \n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        val_rain_losses = []\n",
    "        val_t2m_losses = []\n",
    "        for example in val_dataloader:\n",
    "            t2m_dist, tp_dist = model(example)\n",
    "            \n",
    "            obs_t2m, obs_tp = example['obs_t2m'], example['obs_tp']\n",
    "            \n",
    "            tp_obs = example['obs_tp']\n",
    "            tp_nan_mask = tp_obs.isnan()\n",
    "            tp_obs[tp_nan_mask] = 0.0\n",
    "            tp_log_likelihood = tp_dist.log_prob(tp_obs)\n",
    "            tp_log_likelihood[tp_nan_mask] = 0.0\n",
    "\n",
    "            t2m_obs = example['obs_t2m']\n",
    "            t2m_nan_mask = t2m_obs.isnan()\n",
    "            t2m_obs[t2m_nan_mask] = 0.0\n",
    "            t2m_log_likelihood = t2m_dist.log_prob(t2m_obs)\n",
    "            t2m_log_likelihood[t2m_nan_mask] = 0.0\n",
    "            \n",
    "            val_rain_loss = -tp_log_likelihood.mean()\n",
    "            val_temperature_loss =  -t2m_log_likelihood.mean()\n",
    "            val_loss = val_rain_loss + val_temperature_loss\n",
    "            \n",
    "            val_rain_losses.append(val_rain_loss.detach())\n",
    "            val_t2m_losses.append(val_temperature_loss.detach())\n",
    "            val_losses.append(val_loss.detach())\n",
    "        \n",
    "        \n",
    "    val_mean_loss = np.array(val_losses).mean()\n",
    "    val_mean_rain_loss = np.array(val_rain_losses).mean()\n",
    "    val_mean_temperature_loss = np.array(val_t2m_losses).mean()\n",
    "    print(f'Epoch {epoch} val loss: {val_mean_loss}. Temperature: {val_mean_temperature_loss}. Rain: {val_mean_rain_loss}.')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_re = re.compile('01[0-9]{2}.nc$')\n",
    "#filter_re = re.compile('0109.nc$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_val_dataset = S2SDataset(DATASET, include_features=False, name_filter=lambda x: filter_re.search(x), years=list(range(2017,2020)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = CompositeTransform([add_biweekly_dim_transform, linear_model_adapter, add_metadata])\n",
    "val_dataset = TransformedDataset(raw_val_dataset, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have to specify collate_fn `lambda x: x` because if we set it at none, pytorch will use it's own collate_fn, which mangles the datasets and turns them into dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, num_workers=4, batch_size=None, batch_sampler=None, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges_cdf_to_terciles(edges_cdf):\n",
    "    return torch.stack([\n",
    "        edges_cdf[0],\n",
    "        edges_cdf[1] - edges_cdf[0],\n",
    "        1.0 - edges_cdf[1],\n",
    "    ], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edges_cdf_from_distribution(distribution, edges):\n",
    "    edges_nan_mask = edges.isnan()\n",
    "    edges[edges_nan_mask] = 0.0\n",
    "    cdf = distribution.cdf(edges)\n",
    "    edges[edges_nan_mask] = np.nan\n",
    "    cdf[t2m_edges_nan_mask] = np.nan\n",
    "    \n",
    "    return cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terciles_pytorch_to_xarray(t2m, tp, example_dataset, dims=['category', 'lead_time', 'latitude', 'longitude']):    \n",
    "    t2m_array = xr.DataArray(\n",
    "        data=t2m.detach().numpy(), \n",
    "        dims=dims,\n",
    "        name='t2m'\n",
    "    )\n",
    "    tp_array = xr.DataArray(\n",
    "        data=tp.detach().numpy(),\n",
    "        dims=dims,\n",
    "        name='tp'\n",
    "    )\n",
    "    dataset = xr.Dataset(data_vars={\n",
    "        't2m': t2m_array,\n",
    "        'tp': tp_array,\n",
    "    })\n",
    "\n",
    "    dataset = dataset.assign_coords({\n",
    "        'forecast_year': example_forecast.forecast_year.data,\n",
    "        'forecast_monthday': example_forecast.forecast_monthday.data,\n",
    "        'lead_time': example_forecast.lead_time.data,\n",
    "        'valid_time': example_forecast.valid_time,\n",
    "        'forecast_time': example_forecast.forecast_time.data,\n",
    "        'latitude': example_forecast.latitude.data,\n",
    "        'longitude': example_forecast.longitude.data,\n",
    "        'category': ['below normal', 'near normal', 'above normal'],\n",
    "    }).expand_dims(['forecast_year', 'forecast_monthday'])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    datasets_of_examples = []\n",
    "    \n",
    "    for example in val_dataloader:\n",
    "        pytorch_example = example_to_pytorch(example)\n",
    "        t2m_dist, tp_dist = model(pytorch_example)\n",
    "\n",
    "        t2m_edges = torch.cat([torch.full((2, 1, 121, 240), np.nan), pytorch_example['edges_t2m']], 1)\n",
    "        t2m_cdf = compute_edges_cdf_from_distribution(t2m_dist, t2m_edges)\n",
    "\n",
    "        tp_edges = torch.cat([torch.full((2, 1, 121, 240), np.nan), pytorch_example['edges_tp']], 1)\n",
    "        tp_cdf = compute_edges_cdf_from_distribution(tp_dist, tp_edges)\n",
    "\n",
    "        t2m_terciles = edges_cdf_to_terciles(t2m_cdf)\n",
    "        tp_terciles = edges_cdf_to_terciles(tp_cdf)\n",
    "\n",
    "        example_forecast = example['model']\n",
    "        \n",
    "        dataset = terciles_pytorch_to_xarray(t2m_terciles, tp_terciles, example_forecast)\n",
    "        datasets_of_examples.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_prediction = xr.combine_by_coords(datasets_of_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.t2m.isel(category=2, lead_time=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_array = xr.DataArray(data=t2m_terciles.detach().numpy(), dims=['tercile', 'lead_time', 'latitude', 'longitude'], )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_terciles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_example['edges_t2m'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cax = plt.imshow(t2m_cdf[0,1].detach().numpy() < t2m_cdf[1,1].detach().numpy())\n",
    "plt.colorbar(cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cax = plt.imshow(t2m_cdf[0,2].detach().numpy())\n",
    "plt.colorbar(cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cax = plt.imshow(t2m_terciles[2,2].detach().numpy())\n",
    "plt.colorbar(cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S2S Challenge",
   "language": "python",
   "name": "s2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
