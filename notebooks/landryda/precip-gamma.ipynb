{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "\n",
    "from crims2s.dataset import S2SDataset, TransformedDataset\n",
    "from crims2s.transform import AddBiweeklyDimTransform\n",
    "from crims2s.distribution import std_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = '***BASEDIR***/mlready/2021-08-28-test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = S2SDataset(DATASET, include_features=False, name_filter=lambda x: x.endswith('0102.nc'))\n",
    "dataset = TransformedDataset(dataset, AddBiweeklyDimTransform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[10].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dataset[10]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xr.concat([dataset[i]['model'] for i in range(4)], dim='forecast_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit gamma using pytorch\n",
    "\n",
    "The fitting is too slow using scipy stats. We'll have to make that logic using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REG = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_total = model.tp.isel(lead_time=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial estimate using the method of moments.\n",
    "\n",
    "a_hat_xarray = weekly_total.mean(dim='realization') ** 2 / (weekly_total.var(dim='realization') + REG)\n",
    "b_hat_xarray = (weekly_total.mean(dim='realization') + REG) / (weekly_total.var(dim='realization') + REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a_hat_xarray / b_hat_xarray**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tp = a_hat_xarray / b_hat_xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tp.isel(biweekly_forecast=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat =torch.tensor(a_hat_xarray.data, requires_grad=True)\n",
    "b_hat = torch.tensor(b_hat_xarray.data, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_total_torch = torch.clamp(torch.from_numpy(weekly_total.transpose('realization', 'biweekly_forecast', 'latitude', 'longitude').data), min=REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_total_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([a_hat, b_hat], lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_total_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    estimated_gamma = torch.distributions.Gamma(torch.clamp(a_hat, min=REG) , torch.clamp(b_hat, min=REG))\n",
    "\n",
    "    mean_log_likelihood = estimated_gamma.log_prob(weekly_total_torch).mean()\n",
    "\n",
    "    mean_lls.append(-mean_log_likelihood.detach().item())\n",
    "\n",
    "    loss = -mean_log_likelihood\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_lls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((a_hat.detach() / b_hat.detach())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_hat.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a_hat / b_hat).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gamma_xarray(array: xr.DataArray, dim=None, regularization=1e-9, **kwargs):\n",
    "    \"\"\"\"\"\"\n",
    "    # Use method of moments for initial estimate.\n",
    "    a_hat_xarray = array.mean(dim=dim) ** 2 / (array.var(dim=dim) + regularization)\n",
    "    b_hat_xarray = (array.mean(dim=dim) + regularization) / (array.var(dim=dim) + regularization)\n",
    "    \n",
    "    transposed = array.transpose(dim, ...)\n",
    "    \n",
    "    alpha, beta = fit_gamma_pytorch(transposed.data, a_hat_xarray.data, b_hat_xarray.data, regularization=regularization, **kwargs)\n",
    "    \n",
    "    alpha_xarray = xr.zeros_like(a_hat_xarray).rename(f'{a_hat_xarray.name}_alpha')\n",
    "    beta_xarray = xr.zeros_like(b_hat_xarray).rename(f'{a_hat_xarray.name}_beta')\n",
    "    \n",
    "    alpha_xarray.data = alpha.numpy()\n",
    "    beta_xarray.data = beta.numpy()\n",
    "    \n",
    "    return xr.merge([alpha_xarray, beta_xarray])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gamma_pytorch(data, a_hat, b_hat, regularization=1e-9, max_epochs=500, lr=1e-2, tol=1e-5, patience=5, return_losses=False):\n",
    "    n_iter_waited = 0\n",
    "    \n",
    "    alpha = torch.tensor(a_hat, requires_grad=True)\n",
    "    beta = torch.tensor(b_hat, requires_grad=True)\n",
    "    data = torch.tensor(data)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([alpha, beta], lr=lr)\n",
    "    log_likelihoods = []\n",
    "    for epoch in range(max_epochs):\n",
    "        clamped_alpha = torch.clamp(alpha, min=regularization)\n",
    "        clamped_beta = torch.clamp(beta, min=regularization)\n",
    "        \n",
    "        estimated_gamma = torch.distributions.Gamma(clamped_alpha , clamped_beta)\n",
    "\n",
    "        loss = -estimated_gamma.log_prob(data).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if len(log_likelihoods) > 0:\n",
    "            percent_improvement = log_likelihoods[-1] / loss - 1.0\n",
    "            best_loss = np.array(log_likelihoods).min()\n",
    "            if np.abs(best_loss - loss.detach()) < tol:\n",
    "                n_iter_waited += 1\n",
    "                \n",
    "                if n_iter_waited >= patience:\n",
    "                    break\n",
    "        \n",
    "        log_likelihoods.append(loss.detach().item())\n",
    "            \n",
    "    alpha, beta = torch.clamp(alpha, min=regularization).detach(), torch.clamp(beta, min=regularization).detach()\n",
    "\n",
    "    if return_losses:\n",
    "        return alpha, beta, log_likelihoods\n",
    "    else:\n",
    "        return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_params = fit_gamma_xarray(model.tp.isel(lead_time=-1).clip(min=1e-9), dim='realization', tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_params.tp_beta.isel(biweekly_forecast=1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero inflated gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalExpMixture:\n",
    "    def __init__(self, loc, scale, rate, mix):\n",
    "        self.normal = torch.distributions.Normal(loc, scale)\n",
    "        self.exponential = torch.distributions.Exponential(rate)\n",
    "        self.mix = mix\n",
    "        \n",
    "    def log_prob(self, x):\n",
    "        return (1.0 - self.mix) * self.normal.log_prob(x) + self.mix * self.exponential.log_prob(x + 1e-9)\n",
    "    \n",
    "    def cdf(self, x):\n",
    "        return (1.0 - self.mix) * self.normal.cdf(x) + self.mix * self.exponential.cdf(x + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureModel:\n",
    "    def __init__(self, loc, scale, zero):\n",
    "        self.normal = torch.distributions.Normal(loc, scale)\n",
    "        self.zero = torch.tensor(zero)\n",
    "        \n",
    "    def __check_input(self, x):\n",
    "        if (x < 0.0).any():\n",
    "            raise ValueError('Values must be whithin support')\n",
    "            \n",
    "    def log_prob(self, x):\n",
    "        self.__check_input(x)\n",
    "        \n",
    "        ll = torch.zeros_like(x)\n",
    "        \n",
    "        zero_mask = x == 0.0\n",
    "        \n",
    "        print(ll.shape)\n",
    "        print(self.zero.shape)\n",
    "        \n",
    "        ll[:] = self.zero\n",
    "        ll[~zero_mask] = (1.0 - self.zero) * self.normal.log_prob(x)[~zero_mask]\n",
    "        \n",
    "        return ll\n",
    "    \n",
    "    def cdf(self, x):\n",
    "        self.__check_input(x)\n",
    "        \n",
    "        zero_mask = x == 0.0\n",
    "        cdf[zero_mask] = self.zero[zero_mask]\n",
    "        cdf[~zero_mask] = self.zero[~zero_mask] + self.normal.cdf(x)[~zero_mask]\n",
    "        \n",
    "        return cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroInflatedGaussian:\n",
    "    def __init__(self, loc, scale):\n",
    "        self.normal = torch.distributions.Normal(loc, scale)\n",
    "        \n",
    "    def log_prob(self, x):\n",
    "        if (x < 0.0).any():\n",
    "            raise ValueError('Values must be whithin support')\n",
    "        \n",
    "        ll = torch.zeros_like(x)\n",
    "        \n",
    "        bigger_mask = x > 0.\n",
    "        ll[bigger_mask] = self.normal.log_prob(x)[bigger_mask]\n",
    "        \n",
    "        equal_mask = x == 0.\n",
    "        ll[equal_mask] = self.normal.cdf(x)[equal_mask]\n",
    "        \n",
    "        return ll\n",
    "        \n",
    "        \n",
    "    def cdf(self, x):\n",
    "        if (x < 0.0).any():\n",
    "            raise ValueError('Values must be whithin support')\n",
    "            \n",
    "        return self.normal.cdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CensoredNormal:\n",
    "    def __init__(self, loc, scale):\n",
    "        self.normal = torch.distributions.Normal(loc, scale)\n",
    "        \n",
    "    def __check_input(self, x):\n",
    "        if (x < 0.0).any():\n",
    "            raise ValueError('Values must be whithin support')\n",
    "        \n",
    "    def log_prob(self, x):\n",
    "        self.__check_input(x)\n",
    "        \n",
    "        normal_log_prob = self.normal.log_prob(x)\n",
    "        denominator = torch.log(1.0 - self.normal.cdf(torch.zeros_like(x)) + 1e-6)\n",
    "\n",
    "        log_prob = normal_log_prob - denominator \n",
    "                \n",
    "        return log_prob\n",
    "        \n",
    "    def cdf(self, x):\n",
    "        self.__check_input(x)\n",
    "        zero_cdf = self.normal.cdf(torch.zeros_like(x))\n",
    "        \n",
    "    \n",
    "        \n",
    "        return (self.normal.cdf(x) - zero_cdf) / (1.0 - zero_cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_zero_inflated_normal_pytorch(data, mu_hat, theta_hat, regularization=1e-9, max_epochs=1, lr=1e-4, tol=1e-5, patience=5, log_likelihoods=losses):\n",
    "    n_iter_waited = 0\n",
    "    \n",
    "    mu = torch.tensor(mu_hat, requires_grad=True)\n",
    "    mu.retain_grad()\n",
    "    theta = torch.tensor(theta_hat, requires_grad=True)\n",
    "    \n",
    "    data = torch.tensor(data)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([mu, theta], lr=lr)\n",
    "    for epoch in range(max_epochs):\n",
    "        clampe\n",
    "        clamped_theta = torch.clamp(theta, min=regularization)\n",
    "        clamped_theta.retain_grad()\n",
    "                \n",
    "        estimated_distribution = CensoredNormal(mu, clamped_theta)\n",
    "        \n",
    "        loss = -estimated_distribution.log_prob(data).mean() + torch.square(mu).mean()\n",
    "        loss.backward()\n",
    "                \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if len(log_likelihoods) > 0:\n",
    "            percent_improvement = log_likelihoods[-1] / loss - 1.0\n",
    "            best_loss = np.array(log_likelihoods).min()\n",
    "            if np.abs(best_loss - loss.detach()) < tol:\n",
    "                n_iter_waited += 1\n",
    "                \n",
    "                if n_iter_waited >= patience:\n",
    "                    break\n",
    "        \n",
    "        log_likelihoods.append(loss.detach().item())\n",
    "            \n",
    "    mu, theta = mu.detach(), theta.detach()\n",
    "\n",
    "    return mu, torch.clamp(theta, min=regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_hat = weekly_total.mean(dim=['realization', 'forecast_time'])\n",
    "theta_hat = std_estimator(weekly_total, dim=['realization', 'forecast_time']) + 1.0\n",
    "\n",
    "tp_data = weekly_total.transpose('realization', 'forecast_time', ...).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "mu, theta = fit_zero_inflated_normal_pytorch(tp_data, mu_hat.data, theta_hat.data, log_likelihoods=losses, lr=1e-1, tol=1e-4, max_epochs=1000, regularization=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=theta[mu < 0].detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(theta[mu < 0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=mu.detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=mu[(mu > 0) & (mu < 1)].detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=mu[mu < 0].detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=theta.detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate[mix > 0.95].histc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=rate[mix > 0.8].detach().numpy().flatten(), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=mu[mix < 0.1].detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mu[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_total.isel(biweekly_forecast=0, realization=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = NormalExpMixture(4.0, 3.0, 1.0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.exp(distr.log_prob(torch.arange(0.0, 10.0, step=1e-2))).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_zero_inflated_normal_pytorch(data, mu_hat, theta_hat, rate_hat, regularization=1e-9, max_epochs=1, lr=1e-2, tol=1e-5, patience=5, return_losses=False):\n",
    "    n_iter_waited = 0\n",
    "    \n",
    "    mu = torch.tensor(mu_hat, requires_grad=True)\n",
    "    theta = torch.tensor(theta_hat, requires_grad=True)\n",
    "    rate = torch.tensor(rate_hat, requires_grad=True)\n",
    "    #mix = torch.tensor(rate_hat, requires_grad=True)\n",
    "    mix = torch.zeros_like(rate, requires_grad=True)\n",
    "    \n",
    "    data = torch.tensor(data)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([mu, theta, rate, mix], lr=lr)\n",
    "    log_likelihoods = []\n",
    "    for epoch in range(max_epochs):\n",
    "        clamped_theta = torch.clamp(theta, min=regularization)\n",
    "        clamped_rate = torch.clamp(rate, min=regularization)\n",
    "        clamped_mix = torch.sigmoid(mix)\n",
    "        \n",
    "        estimated_distribution = NormalExpMixture(mu, clamped_theta, clamped_rate, clamped_mix)\n",
    "\n",
    "        loss = -estimated_distribution.log_prob(data).mean()\n",
    "\n",
    "        loss.backward()\n",
    "                \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if len(log_likelihoods) > 0:\n",
    "            percent_improvement = log_likelihoods[-1] / loss - 1.0\n",
    "            best_loss = np.array(log_likelihoods).min()\n",
    "            if np.abs(best_loss - loss.detach()) < tol:\n",
    "                n_iter_waited += 1\n",
    "                \n",
    "                if n_iter_waited >= patience:\n",
    "                    break\n",
    "        \n",
    "        log_likelihoods.append(loss.detach().item())\n",
    "            \n",
    "    mu, theta = mu.detach(), theta.detach()\n",
    "\n",
    "    if return_losses:\n",
    "        return mu, torch.clamp(theta, min=regularization), torch.clamp(rate, min=regularization), torch.sigmoid(mix), log_likelihoods\n",
    "    else:\n",
    "        return mu, torch.clamp(theta, min=regularization), torch.clamp(rate, min=regularization), torch.sigmoid(mix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S2S Challenge",
   "language": "python",
   "name": "s2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
