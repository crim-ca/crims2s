{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debias precipitation using a gamma distribution\n",
    "\n",
    "So same thing as the other debiasing notebooks, but this time we use a gamma distribution instead of a gaussian.\n",
    "This should lead to a slightly better debiasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.distributed\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import scipy.stats\n",
    "import typing\n",
    "import xarray as xr\n",
    "import xskillscore as xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crims2s.dask import create_dask_cluster\n",
    "from crims2s.util import fix_dataset_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TRAIN = '***BASEDIR***training-input/0.3.0/netcdf'\n",
    "OBSERVATIONS = '***BASEDIR***/processed/training-output-reference/'\n",
    "BENCHNMARK = '***BASEDIR***training-output-benchmark/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boost dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = create_dask_cluster()\n",
    "cluster.scale(jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_validation_from_lead_time(xr_data) -> typing.Tuple:\n",
    "    xr_data_sub_train = xr_data.sel(forecast_year=slice(None, 2018))\n",
    "    xr_data_sub_val = xr_data.sel(forecast_year=slice(2019, None))\n",
    "    \n",
    "    return xr_data_sub_train, xr_data_sub_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_correct_bias(data_center_train, data_center_val, obs_train):\n",
    "    \n",
    "    bias = (obs_train - data_center_train).mean(dim=['lead_time', 'forecast_year'])\n",
    "    corrected_bias = data_center_val + bias\n",
    "    \n",
    "    return bias, corrected_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_biweekly_dim(dataset):\n",
    "    weeklys = []\n",
    "    for s in [slice('0D', '13D'), slice('14D', '27D'), slice('28D', '41D')]:\n",
    "        weekly_forecast = dataset.sel(lead_time=s)\n",
    "\n",
    "        first_lead = pd.to_timedelta(weekly_forecast.lead_time[0].item())\n",
    "\n",
    "        weekly_forecast = weekly_forecast.expand_dims(dim='biweekly_forecast').assign_coords(biweekly_forecast=[first_lead])\n",
    "        weekly_forecast = weekly_forecast.assign_coords(lead_time=(weekly_forecast.lead_time - first_lead))\n",
    "        weeklys.append(weekly_forecast)\n",
    "        \n",
    "    weeklys[2] -= weeklys[1].isel(lead_time=-1)\n",
    "    weeklys[1] -= weeklys[0].isel(lead_time=-1)\n",
    "        \n",
    "    return xr.concat(weeklys, dim='biweekly_forecast').transpose('forecast_year', 'forecast_dayofyear', 'biweekly_forecast', ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CENTER = 'ncep'\n",
    "FIELD = 'tp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = pathlib.Path(INPUT_TRAIN)\n",
    "input_files_tp = sorted([f for f in input_path.iterdir() if CENTER in f.stem and FIELD in f.stem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files_tp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecmwf_tp_raw = xr.open_mfdataset(input_files_tp, preprocess=fix_dataset_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecmwf_tp_raw.isel(lead_time=0, realization=0, forecast_year=0, forecast_dayofyear=0).tp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are non-zero values on the first lead time, so it's the values accumulated after 24h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecmwf_tp = add_biweekly_dim(ecmwf_tp_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecmwf_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_smaller = (ecmwf_tp.isel(lead_time=-1) < ecmwf_tp.isel(lead_time=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_smaller.sum(dim=['realization', 'latitude', 'longitude', 'forecast_dayofyear', 'forecast_year']).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_path = pathlib.Path(OBSERVATIONS)\n",
    "obs_files = [f for f in obs_path.iterdir() if 'tp' in f.stem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_files[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tp_raw = xr.open_mfdataset(obs_files)\n",
    "obs_tp_raw = obs_tp_raw.assign_coords(lead_time=obs_tp_raw.lead_time - obs_tp_raw.lead_time[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tp = add_biweekly_dim(obs_tp_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tp = obs_tp.isel(lead_time=-1) - obs_tp.isel(lead_time=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecmwf_tp_train, ecmwf_tp_val = extract_train_validation_from_lead_time(ecmwf_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tp_train, obs_tp_val = extract_train_validation_from_lead_time(obs_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecmwf_tp_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecmwf_tp_train.isel(biweekly_forecast=1, forecast_dayofyear=10, latitude=30, longitude=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tp_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Gamma distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_slice = ecmwf_tp_train.isel(biweekly_forecast=1, forecast_dayofyear=0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(one_slice.tp < 0.0).sum(dim=['forecast_year', 'realization']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.gamma.fit(one_slice.tp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.apply_ufunc(scipy.stats.gamma.fit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S2S Challenge",
   "language": "python",
   "name": "s2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
